\documentclass[../main.tex]{subfiles}

\begin{document}

\chapter{代码主体结构}
\vspace{-2cm}

在本章节中，我们将对 BasicSR 代码框架进行一个整体介绍，主要包括以下内容：整体框架 (第\ref{code_structure:overview}小节)、配置文件 (第\ref{code_structure:config}小节)、注册器机制 (第\ref{code_structure:register}小节)、数据 (第\ref{code_structure:data}小节)、网络结构 (第\ref{code_structure:arch}小节)、模型 (第\ref{code_structure:model}小节)、损失函数 (第\ref{code_structure:loss}小节)、训练 (第\ref{code_structure:training}小节)、算子 (第\ref{code_structure:ops}小节) 等。通过阅读本章，你将对 BasicSR 有进一步的认识，理解其模块之间的相互关系、以及模块内部的核心工作原理。但我们不对具体函数和代码做具体介绍。如果需要具体函数和代码的介绍，请查阅 BasicSR 的在线 API 文档 (\url{https://basicsr.readthedocs.io/en/latest/})。

% ------------------------------------------------------------------------------

\section{整体框架} \label{code_structure:overview}
对于基于深度学习的算法框架，其核心的组成部分包括：\textbf{数据、模型、损失函数、训练}。BasicSR 框架也是大致根据以上部分撰写而成的。下图概括了 BasicSR 的整体组成框架：

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=1\linewidth]{figures/main_framework.pdf}
    \end{center}
    \caption{BasicSR 代码整体框架}
    \label{fig:main_framework}
\end{figure}

\begin{itemize}
    \item 数据 (Data)：这个部分主要定义了 Dataset 和 Data Loader 文件, 放在了 \href{https://github.com/XPixelGroup/BasicSR/tree/master/basicsr/data}{data} 目录下。Dataset 用于读取和预处理数据，包括图像读取、归一化 (normalization)、数据增强 (augmentation) 以及封装为 PyTorch Tensor 等。同时，我们也提供了一些辅助函数，帮助使用者自定义自己的数据预处理功能，例如图像色彩空间转换、常用 MATLAB 函数的 Python 版本、常用的图像退化模型 (degradation model) 等。详细说明参见章节\ref{code_structure:data}：\nameref{code_structure:data}。

    \item 模型 (Model)：在 \href{https://github.com/XPixelGroup/BasicSR/tree/master/basicsr/models}{models} 目录下，我们提供了常用的模型文件。这些模型文件主要用于定义网络结构与初始化、输入输出数据、一次 forward 的训练过程、保存加载模型等。在 \href{https://github.com/XPixelGroup/BasicSR/tree/master/basicsr/archs}{archs} 目录下，我们提供了常用的网络结构模型文件，包括 SRResNet、ESRGAN、RCAN、SwinIR、EDVR、BasicVSR 等。在 \href{https://github.com/XPixelGroup/BasicSR/tree/master/basicsr/losses}{losses} 文件夹中，我们提供了常用的损失函数，例如 L1/L2 loss、perceptual loss、GAN loss 等。详细说明参见章节\ref{code_structure:arch}：\nameref{code_structure:arch}、章节\ref{code_structure:model}：\nameref{code_structure:model}、章节\ref{code_structure:loss}：\nameref{code_structure:loss}。

    \item 配置 (Option)：配置文件放到 \href{https://github.com/XPixelGroup/BasicSR/tree/master/options}{option} 目录下。我们提供了常用模型的训练和测试配置文件。我们使用 \href{https://yaml.org/}{YAML} 来作为配置文件的语言。修改这些 yml 文件可以简易地调整训练过程中的各种超参数。详细说明参见章节\ref{code_structure:config}：\nameref{code_structure:config}和章节\ref{code_structure:register}：\nameref{code_structure:register}。

    \item 训练 (Training)：这一部分主要涉及训练的策略和记录训练日志。\href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/train.py}{train.py} 和 \href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/test.py}{test.py} 是启动模型训练和测试的入口文件，其中定义了训练和测试的 main loop。常见优化器 (optimizer) 的定义可以在 \href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/models/base_model.py}{models/base\_model.py} 文件中的 \texttt{get\_optimizer} 函数中找到。学习率的调度策略在 \href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/models/base_model.py}{models/base\_model.py}  文件中的 \texttt{setup\_schedulers} 函数中定义。为了方便追踪记录训练的过程，我们提供了相应的 logger 工具，支持直接 print 到屏幕、Tensorboard、Wandb等多种方式，具体代码可以在 \href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/utils/logger.py}{utils/logger.py} 中找到。详细说明参见章节\ref{code_structure:training}：\nameref{code_structure:training}。\todo{本章节需要添加 training pipeline、logging 使用方式、test pipeline 的介绍。还有比如 logger 怎么用，validation 放在哪里、怎么用的内容。安排：logging 和 validation 放到 4.5 节 模型里面。training/testing pipeline 放到 4.7 节这里，和入门呼应、互补}

    \item 详细的代码接口文档可以在 \url{http://basicsr.readthedocs.io} 查询。
\end{itemize}

\begin{note} % ---------------- Note block ---------------- %
    \textbf{BasicSR 目录说明}

    你可以在章节\ref{getting_start:content-overview}：\nameref{getting_start:content-overview} 查看到详细的 BasicSR 目录说明。
\end{note}

% ------------------------------------------------------------------------------

\section{配置(Options)}\label{code_structure:config}

在这个章节，我们先简单介绍一下实验命名的约定 (第\ref{code_structure:name_convention}小节)；然后通过例子介绍训练和测试的配置文件 (第\ref{code_structure:config_example}小节)。

% ----------------------------------
\subsection{实验命名}\label{code_structure:name_convention}

我们推荐对实验名字进行有意义的命名，方便后续的实验以及进行多组实验对比。

我们以 \texttt{001\_MSRResNet\_x4\_f64b16\_DIV2K\_1000k\_B16G1\_wandb} 为例:

\begin{itemize}
\item 001: 我们一般给实验进行数字打头的标号, 方便进行实验管理
\item MSRResNet: 模型名称, 这里指代 Modified SRResNet
\item x4\_f64b16: 重要配置参数, 这里表示放大4倍; 中间feature通道数是64, 使用了16个Residual Block
\item DIV2K: 训练数据集是DIV2K
\item 1000k: 训练了1000k iterations
\item B16G1: Batch size 为16, 使用一卡 GPU 训练
\item wandb: 使用了 wandb, 训练过程上传到了 wandb 云服务器
\end{itemize}

\begin{hl} % ---------------- Highlight block ---------------- %
    \textbf{注意}

    如果在实验名字中有 \texttt{debug} 字样, 则会进入 debug 模式, 即程序会更密集地 log 和 validate, 并且不会使用 tensorboard logger 和 wandb logger。

    具体参见章节\ref{others:debug_mode}：\nameref{others:debug_mode}。
\end{hl}

% ----------------------------------
\subsection{配置文件简要说明}\label{code_structure:config_example}

在 BasicSR 中我们使用 \href{https://yaml.org/}{YAML} 来作为配置文件的语言。

训练的配置文件在 \href{https://github.com/XPixelGroup/BasicSR/tree/master/options/train}{options/train} 中，测试的配置文件在 \href{https://github.com/XPixelGroup/BasicSR/tree/master/options/test}{options/test} 中。
通过 option 配置文件，我们可以设置实验名、选择模型、指定 GPU、指定数据路径、选择网络结构、配置训练策略等。

我们在第\ref{code_structure:train_config}小节介绍训练配置文件例子，在第\ref{code_structure:test_config}小节介绍测试配置文件例子。

\todo{TODO：添加简要代码解析 - 如何从 YAML 文件解析。即分析 \href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/utils/options.py}{basicsr/utils/options.py}}。

\subsubsection{训练配置文件例子}\label{code_structure:train_config}

下面，我们以 \href{https://github.com/XPixelGroup/BasicSR/blob/master/options/train/SRResNet_SRGAN/train_MSRResNet_x4.yml}{train\_MSRResNet\_x4.yml} 为例，简单说明训练配置文件的每个部分。我们先把配置文件贴出来，在后面附上解释。然后在说明框内会列举相关的要点。为方便说明，整个配置文件会被分散成不同的板块来讲解。

\begin{minted}[xleftmargin=20pt,breaklines,bgcolor=bg]{python}
# Modified SRResNet w/o BN from:
# Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network

# ----------- Commands for running
# ----------- Single GPU with auto_resume
# PYTHONPATH="./:${PYTHONPATH}"  CUDA_VISIBLE_DEVICES=0 python basicsr/train.py -opt options/train/SRResNet_SRGAN/train_MSRResNet_x4.yml --auto_resume

# general settings - 这块为通用设置
name: 001_MSRResNet_x4_f64b16_DIV2K_1000k_B16G1_wandb  # 实验名称, 若实验名字中有debug字样, 则会进入debug模式
model_type: SRModel  # 使用的 model 类型
scale: 4  # 输出比输入的倍数, 在SR中是放大倍数; 若有些任务没有这个配置, 则写1
num_gpu: 1  # 指定使用的 GPU 卡数
manual_seed: 0  # 指定随机种子
\end{minted}

\begin{exampleBox}[righthand ratio=0.00, sidebyside, sidebyside align=center, lower separated=false]{通用配置}
\begin{itemize}
    \item 在配置文件的最开始，会有简单的说明，以及默认的运行命令。运行命令的 \texttt{auso\_resume} 表示自动从断点接着训练 (详见章节\ref{others:auto_resume}：\nameref{others:auto_resume})。
    \item 常见模型 (Model) 的定义在 \href{https://github.com/XPixelGroup/BasicSR/tree/master/basicsr/models}{models} 目录中。详细说明参见章节\ref{code_structure:model}：\nameref{code_structure:model}。
    \item \texttt{num\_gpu}：\textbf{0 表示 使用CPU，\texttt{auto} 表示自动从可用 GPU 块数推断}。
\end{itemize}
\end{exampleBox}

\begin{minted}[xleftmargin=20pt,bgcolor=bg,breaklines]{python}
# dataset and data loader settings
datasets:  # 这块是 dataset 的配置
  train:  # 训练 dataset 的配置
    name: DIV2K  # 自定义的数据集名称
    type: PairedImageDataset  # 读取数据的 Dataset 类
    # 以下属性是灵活的, 可在相应类的说明文档中获得。新加的数据集可根据需要添加
    dataroot_gt: datasets/DF2K/DIV2K_train_HR_sub  #  GT (Ground-Truth) 图像的文件夹路径
    dataroot_lq: datasets/DF2K/DIV2K_train_LR_bicubic_X4_sub  # LQ (Low-Quality) 输入图像的文件夹路径
    meta_info_file: basicsr/data/meta_info/meta_info_DIV2K800sub_GT.txt  # 预先生成的 meta_info 文件
    # (for lmdb)
    # dataroot_gt: datasets/DIV2K/DIV2K_train_HR_sub.lmdb
    # dataroot_lq: datasets/DIV2K/DIV2K_train_LR_bicubic_X4_sub.lmdb
    filename_tmpl: '{}'  # 文件名字模板, 一般LQ文件会有类似 '_x4' 这样的文件后缀, 这个就是来处理GT和LQ文件后缀不匹配的问题的
    io_backend:  # IO 读取的 backend
      type: disk  # disk 表示直接从硬盘读取
      # (for lmdb)
      # type: lmdb

    gt_size: 128  # 训练阶段裁剪 (crop) 的GT图像的尺寸大小，即训练的 label 大小
    use_hflip: true  # 是否开启水平方向图像增强 (随机水平翻转图像)
    use_rot: true  # 是否开启旋转图像增强 (随机旋转图像)

    # data loader - 下面这块是 data loader 的设置
    num_worker_per_gpu: 6  # 每一个 GPU 的 data loader 读取进程数目
    batch_size_per_gpu: 16  # 每块 GPU 上的 batch size
    dataset_enlarge_ratio: 100  # 放大 dataset 的长度倍数 (默认为1)。可以扩大一个 epoch 所需 iterations
    prefetch_mode: ~  # 预先读取数据的方式
\end{minted}
\begin{exampleBox}[righthand ratio=0.00, sidebyside, sidebyside align=center, lower separated=false]{数据读取相关配置}
\begin{itemize}
    \item 常见数据 (dataset) 的定义在 \href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/data}{basicsr/data} 目录中。详细说明参见章节\ref{code_structure:data}：\nameref{code_structure:data}。
    \item data loader 定义在 \href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/data/__init__.py}{basicsr/data/\_\_init\_\_.py} 文件中。
    \item meta\_info\_file：细节请参看章节\ref{data_preparation:meta_info}：\nameref{data_preparation:meta_info}。\todo{更新链接}
    \item io\_backend：细节请参看章节\ref{data_preparation:meta_info}：\nameref{data_preparation:meta_info}。\todo{更新链接}
    \item dataset\_enlarge\_ratio：详情参见章节\ref{code_structure:data}：\nameref{code_structure:data}。\todo{这块内容需要补足到 数据 这个章节}。
    \item prefetch\_mode:，\textbf{默认为 None，即 $\sim$。\texttt{cpu} 表示使用 CPU prefetcher。\texttt{cuda} 表示使用 CUDA prefetcher。它会多占用一些GPU显存. 注意: 这个模式下, 一定要设置 \texttt{pin\_memory=True}}。详情参见章节\ref{code_structure:data}：\nameref{code_structure:data}。\todo{这块内容需要补足到 数据 这个章节}
\end{itemize}
\end{exampleBox}

\begin{minted}[xleftmargin=20pt,bgcolor=bg,breaklines]{python}
  val:  # validation 数据集的设置
    name: Set5  # 数据集名称
    type: PairedImageDataset  # 数据集的类型
    # 以下属性是灵活的, 类似训练数据集
    dataroot_gt: datasets/Set5/GTmod12
    dataroot_lq: datasets/Set5/LRbicx4
    io_backend:
      type: disk

  val_2:  # 另外一个 validation 数据集
    name: Set14
    type: PairedImageDataset
    dataroot_gt: datasets/Set14/GTmod12
    dataroot_lq: datasets/Set14/LRbicx4
    io_backend:
      type: disk
\end{minted}

\begin{exampleBox}[righthand ratio=0.00, sidebyside, sidebyside align=center, lower separated=false]{validation 配置}
\begin{itemize}
    \item 这里使用了两个 validation sets，它们通过关键字 \texttt{val}，\texttt{val\_2} 来区分。如果有更多的 validation sets，可以通过 \texttt{val\_3}， \texttt{val\_4} ... 来区分。
\end{itemize}
\end{exampleBox}

\begin{minted}[xleftmargin=20pt,bgcolor=bg,breaklines]{python}
# network structures - 网络结构的设置
network_g:  # 网络 g 的设置
  type: MSRResNet  # 网络结构 (Architecture) 的类型
  # 以下属性是灵活且特定的, 可在相应类的说明文档中获得
  num_in_ch: 3  # 模型输入的图像通道数
  num_out_ch: 3  # 模型输出的图像通道数
  num_feat: 64  # 模型内部的 feature map 通道数
  num_block: 16  # 模型内部基础模块的堆叠数
  upscale: 4  # 上采样倍数
\end{minted}

\begin{exampleBox}[righthand ratio=0.00, sidebyside, sidebyside align=center, lower separated=false]{网络结构相关配置}
\begin{itemize}
    \item 常见网络结构 (arch) 的定义在 \href{https://github.com/XPixelGroup/BasicSR/tree/master/basicsr/archs}{archs} 目录下。详细说明参见章节\ref{code_structure:arch}：\nameref{code_structure:arch}。
    \item 如果模型需要使用多个网络，我们一般以 \texttt{network\_} 打头来命名。比如 我们需要一个 discriminator，命名为  \texttt{network\_d}。
\end{itemize}
\end{exampleBox}

\begin{minted}[xleftmargin=20pt,bgcolor=bg,breaklines]{python}
# path
path:  # 以下为路径和与训练模型、重启训练的设置
  pretrain_network_g: ~  # 预训练模型的路径, 需要以 pth 结尾的模型
  param_key_g: params  # 读取的预训练的参数 key。若需要使用 EMA 模型，需要改成 params_ema
  strict_load_g: true  # 是否严格地根据参数名称一一对应 load 模型参数。如果选择 false，那么模型对于找不到的参数，会随机初始化；如果选择 true，假如存在不对应的参数，会报错提示
  resume_state: ~  # 重启训练的 state 路径, 在 experiments/exp_name/training_states 目录下
\end{minted}

\begin{exampleBox}[righthand ratio=0.00, sidebyside, sidebyside align=center, lower separated=false]{模型路径相关配置}
\begin{itemize}
    \item resume\_state设置后, 会覆盖 pretrain\_network\_g 的设定
    \item ：对于 resume，更多信息可以参考章节\ref{others:auto_resume}：\nameref{others:auto_resume}。
\end{itemize}
\end{exampleBox}

\begin{minted}[xleftmargin=20pt,bgcolor=bg,breaklines]{python}
# training settings
train:  # 这块是训练策略相关的配置
  ema_decay: 0.999  # EMA 更新权重
  optim_g:  # 这块是优化器的配置
    type: Adam  # 选择优化器类型，例如 Adam
    # 以下属性是灵活的, 根据不同优化器有不同的设置
    lr: !!float 2e-4  # 初始学习率
    weight_decay: 0  # 权重衰退参数
    betas: [0.9, 0.99]  # Adam 优化器的 beta1 和 beta2

  scheduler:  # 这块是学习率调度器的配置
    type: CosineAnnealingRestartLR   # 选择学习率更新策略
    # 以下属性是灵活的, 根据学习率 Scheduler 的不同有不同的设置
    periods: [250000, 250000, 250000, 250000]  # Cosine Annealing 的更新周期
    restart_weights: [1, 1, 1, 1]  # Cosine Annealing 每次 Restart 的权重
    eta_min: !!float 1e-7  # 学习率衰退到的最小值

  total_iter: 1000000  # 总共进行的训练迭代次数
  warmup_iter: -1  #  warm up 的迭代次数, 如是-1, 表示没有 warm up

  # losses - 这块是损失函数的设置
  pixel_opt:  # loss 名字，这里表示 pixel-wise loss 的 options
    type: L1Loss  # 选择 loss 函数，例如 L1Loss
    # 以下属性是灵活的, 根据不同损失函数有不同的设置
    loss_weight: 1.0  # 指定 loss 的权重
    reduction: mean  # loss reduction 方式
\end{minted}

\begin{exampleBox}[righthand ratio=0.00, sidebyside, sidebyside align=center, lower separated=false]{训练策略相关配置}

训练策略相关的配置主要分为优化器，学习率调度器，总共训练 iterations，损失函数等。
\begin{itemize}
    \item 关于 EMA，请参考章节\ref{others:ema}：\nameref{others:ema}。
    \item optim\_g，后缀 \texttt{\_g} 表示和 \texttt{network\_g} 中的 \texttt{\_g} 一一对应起来。
    \item \texttt{lr: !!float 2e-4} 中的 \texttt{!!float} 是 YAML 语言语法，表示以 float 解释后面的数字，不然就会以文字来进行解释。
    \item 常见优化器 (optimizer) 的定义可以在 \href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/models/base_model.py}{models/base\_model.py} 文件中的 \texttt{get\_optimizer} 函数中找到。
    \item 学习率的调度策略在 \href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/models/base_model.py}{models/base\_model.py}  文件中的 \texttt{setup\_schedulers} 函数中定义。
    \item 常用的损失函数可以在 \href{https://github.com/XPixelGroup/BasicSR/tree/master/basicsr/losses}{losses} 目录中定义。
\end{itemize}
\end{exampleBox}

\begin{minted}[xleftmargin=20pt,bgcolor=bg,breaklines]{python}
# validation settings
val:  # 这块是 validation 的配置
  val_freq: !!float 5e3  # validation 频率, 每隔 5000 iterations 做一次 validation
  save_img: false  # 否需要在 validation 的时候保存图片

  metrics:  # 这块是 validation 中使用的指标的配置
    psnr:  # metric 名字, 这个名字可以是任意的
      type: calculate_psnr  # 选择指标类型
      # 以下属性是灵活的, 根据不同 metric 有不同的设置
      crop_border: 4  # 计算指标时 crop 图像边界像素范围 (不纳入计算范围)
      test_y_channel: false  # 是否转成在 Y(CbCr) 空间上计算
      better: higher  # 该指标是越高越好，还是越低越好。选择 higher 或者 lower，默认为 higher
    niqe:  # 这是在 validation 中使用的另外一个指标
      type: calculate_niqe
      crop_border: 4
      better: lower  # the lower, the better
\end{minted}

\begin{exampleBox}[righthand ratio=0.00, sidebyside, sidebyside align=center, lower separated=false]{validation 相关配置}
\begin{itemize}
    \item 关于 metrics，请参考章节\ref{metrics:}：\nameref{metrics:}。\todo{更新}
    \item 指标在 \href{https://github.com/XPixelGroup/BasicSR/tree/master/basicsr/metrics}{basicsr/metrics} 目录中定义。
    \item BasicSR 支持在 validation 时使用多个指标，只需要在配置文件中添加配置，比如上面的 \texttt{psnr} 和 \texttt{niqe}。
\end{itemize}
\end{exampleBox}

\begin{minted}[xleftmargin=20pt,bgcolor=bg,breaklines]{python}
# logging settings
logger:  # 这块是 logging 的配置
  print_freq: 100  # 多少次迭代打印一次训练信息
  save_checkpoint_freq: !!float 5e3  # 多少次迭代保存一次模型权重和训练状态
  use_tb_logger: true  # 是否使用 tensorboard logger
  wandb:  # 是否使用 wandb logger
    project: ~  #  wandb 的 project名字。 默认是 None, 即不使用 wandb
    resume_id: ~  # 如果是 resume, 可以输入上次的 wandb id, 则 log 可以接起来
\end{minted}

\begin{exampleBox}[righthand ratio=0.00, sidebyside, sidebyside align=center, lower separated=false]{训练日志相关配置}
\begin{itemize}
    \item 关于 wandb，目前 wandb 只是同步 tensorboard 的内容, 因此要使用 wandb, 必须也同时使用 tensorboard。更多关于 wandb，参见章节\ref{metrics:}：\nameref{metrics:}。\todo{更新}。
\end{itemize}
\end{exampleBox}

\begin{minted}[xleftmargin=20pt,bgcolor=bg,breaklines]{python}
# dist training settings
dist_params:  # distributed training 的设置, 目前只在 Slurm 训练下才需要
  backend: nccl
  port: 29500
\end{minted}

至此，我们对于训练的配置文件有了一个初步的理解了。

\subsubsection{测试配置文件例子}\label{code_structure:test_config}

我们以 \href{https://github.com/XPixelGroup/BasicSR/blob/master/options/test/SRResNet_SRGAN/test_MSRResNet_x4.yml}{test\_MSRResNet\_x4.yml} 为例，简单说明测试配置文件的每个部分。我们先把配置文件贴出来，在后面附上解释。然后在说明框内会列举相关的要点。为方便说明，整个配置文件会被分散成不同的板块来讲解。
由于测试配置文件和训练配置文件很类似，我们将简略地进行讲解。

\begin{minted}[xleftmargin=20pt,breaklines,bgcolor=bg]{python}
# ----------- Commands for running
# ----------- Single GPU
# PYTHONPATH="./:${PYTHONPATH}"  CUDA_VISIBLE_DEVICES=0 python basicsr/test.py -opt options/test/SRResNet_SRGAN/test_MSRResNet_x4.yml

# general settings
name: 001_MSRResNet_x4_f64b16_DIV2K_1000k_B16G1_wandb  # 实验名称
model_type: SRModel  # 使用的 model 类型
scale: 4  # 输出比输入的倍数, 在SR中是放大倍数; 若有些任务没有这个配置, 则写1
num_gpu: 1  # 测试卡数
manual_seed: 0  # 指定随机种子

# test dataset settings
datasets:
  test_1:  # 测试数据集的设置, 后缀1表示第一个测试集
    name: Set5  # 数据集的名称
    type: PairedImageDataset  # 读取数据的 Dataset 类
    # GT 和输入 LQ 的根目录
    dataroot_gt: datasets/Set5/GTmod12
    dataroot_lq: datasets/Set5/LRbicx4
    io_backend:  # IO 读取的 backend
      type: disk  # disk 表示直接从硬盘读取
  test_2:  # 测试数据集的设置, 后缀2表示第二个测试集
    name: Set14
    type: PairedImageDataset
    dataroot_gt: datasets/Set14/GTmod12
    dataroot_lq: datasets/Set14/LRbicx4
    io_backend:
      type: disk

# network structures - 网络结构的设置
network_g:  # 网络 g 的设置
  type: MSRResNet  # 网络结构 (Architecture) 的类型
  # 以下是 MSRResNet 的参数设置
  num_in_ch: 3
  num_out_ch: 3
  num_feat: 64
  num_block: 16
  upscale: 4

# path
path:
  pretrain_network_g: experiments/001_..._wandb/models/net_g_1000000.pth  # 预训练模型的路径, 需要以 pth 结尾的模型
  param_key_g: params  # 读取的预训练的参数 key。若需要使用 EMA 模型，需要改成 params_ema
  strict_load_g: true  # 加载预训练模型时, 是否需要网络参数的名称严格对应

# validation settings - 以下为Validation (也是测试)的设置
val:
  save_img: true # 是否需要在测试的时候保存图片
  suffix: ~  # 对保存的图片添加后缀，如果是 None, 则使用 exp name

  metrics:  # 测试时候使用的 metric
    psnr:  # metric 名字, 这个名字可以是任意的
      type: calculate_psnr  # 选择指标类型
      # 以下属性是灵活的, 根据不同 metric 有不同的设置
      crop_border: 4  # 计算指标时 crop 图像边界像素范围 (不纳入计算范围)
      test_y_channel: false  # 是否转成在 Y(CbCr) 空间上计算
      better: higher  # the higher, the better. Default: higher
    ssim:  # 另外一个指标
      type: calculate_ssim
      crop_border: 4
      test_y_channel: false
      better: higher
\end{minted}

\begin{exampleBox}[righthand ratio=0.00, sidebyside, sidebyside align=center, lower separated=false]{注意}
\begin{itemize}
    \item 如果模型训练的时候开启了 EMA，则在测试的时候需要指定 \texttt{param\_key\_g} 为 \texttt{params\_ema}。否则会出现测试和训练过程中 validation 不匹配的问题。
\end{itemize}
\end{exampleBox}

至此，我们对于测试的配置文件有了一个初步的理解了。

% ------------------------------------------------------------------------------

\section{动态实例化与 REGISTER 注册机制}\label{code_structure:register}

% ----------------------------------
\subsection{REGISTER 注册机制}

首先，来看我们的目的：当我们新写了类 (Class) 或函数时，我们希望可直接在配置文件中指定，然后程序会根据配置文件的类名或函数名，自动查找并实例化。
以开发新的网络结构为例，我们会做以下几件事：
\begin{enumerate}
    \item 写具体的网络结构，它往往是一个Class，并且往往是一个单独的文件
    \item 在配置文件中会指定我们使用哪一个网络结构，往往是通过 Class name 指定
    \item 在训练过程的某一个地方，程序会根据配置文件指定的 Class name，自动实例化相关的类
\end{enumerate}

这里说的 REGISTER 注册机制就是来更简洁地实现上面的第三个步骤的。因为其能够根据配置文件动态地实例化所需要的类或函数，因此这个过程被称为动态实例化 (Dynamic Instantiation)。

BasicSR 的 Register 注册机制参考了 FacebookResearch 的 \href{https://github.com/facebookresearch/fvcore}{fvcore} 仓库的函数，定义了 Registry 类。详细代码可查看 \href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/utils/registry.py}{basicsr/utils/registry.py}。它主要有两个函数：register() 和 get()。

\begin{minted}[xleftmargin=20pt,linenos,bgcolor=bg,breaklines]{python}
class Registry():
    """
    The registry that provides name -> object mapping, to support third-party users' custom modules.
    """
    def __init__(self, name):
        self._name = name
        self._obj_map = {}

    def _do_register(self, name, obj, suffix=None):
        ...
        self._obj_map[name] = obj

    def register(self, obj=None, suffix=None):
        # register() 函数主要用来注册一个实现的类或函数
        if obj is None:
            # used as a decorator
            def deco(func_or_class):
                name = func_or_class.__name__
                self._do_register(name, func_or_class, suffix)
                return func_or_class

            return deco

        # used as a function call
        name = obj.__name__
        self._do_register(name, obj, suffix)

    def get(self, name, suffix='basicsr'):
        # get() 函数主要用来根据配置文件中的类名或函数名来查找对应的实例
        ret = self._obj_map.get(name)
        if ret is None:
            ret = self._obj_map.get(name + '_' + suffix)
        ...
        return ret
\end{minted}

\subsubsection{如何注册新的类？}

在 BasicSR 中，我们定义了五个 REGISTER ，相关定义在 \href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/utils/registry.py}{basicsr/utils/registry.py} 中：
\begin{minted}[xleftmargin=20pt,linenos,bgcolor=bg,breaklines]{python}
DATASET_REGISTRY = Registry('dataset')
ARCH_REGISTRY = Registry('arch')
MODEL_REGISTRY = Registry('model')
LOSS_REGISTRY = Registry('loss')
METRIC_REGISTRY = Registry('metric')
\end{minted}

需要注册的时候，我们
\begin{enumerate}
    \item import 相关的注册器，比如 ARCH\_REGISTRY
    \item 使用 Python 装饰器，即在类/函数前面加上 \texttt{@ARCH\_REGISTRY.register()}
\end{enumerate}
这样 RRDBNet 这个类就被注册上啦。

\begin{minted}[xleftmargin=20pt,linenos,bgcolor=bg,breaklines]{python}
from basicsr.utils.registry import ARCH_REGISTRY
from .arch_util import default_init_weights, make_layer, pixel_unshuffle

@ARCH_REGISTRY.register()
class RRDBNet(nn.Module):
    def __init__(self):
        super(RRDBNet, self).__init__()
        ...
\end{minted}



\subsubsection{如何使用已注册的类？}

当我们需要使用的时候，配置
它能有效解决上面提到的两个问题：

1、注册的时候，会强制检查有没有重名，减少了 bug 的产生。

2、只有在需要时，才会进行注册类或函数，其他中间量不会被注册。





其他的 DATASET，ARCH，MODEL，LOSS 都是类似的操作。它们都是注册了类，实例化的时候根据配置的 Class name 进行实例化。

注意，METRIC 稍微有点特殊，它是注册了函数，一样的用法，但是会根据函数名来调用相对应的函数。后面介绍 METRIC 的时候，再具体展开。

值得注意的是，即使我们使用了 REGISTER 机制，import 问题还是存在，即 Python 怎么知道你写了某个网络结构文件。

为了尽量少修改文件，我们沿用了动态实例化时候的做法，约定网络结构的文件使用  \_arch.py 结尾，然后自动扫描，import 进来。

其他的几个注册器的约定：

1、DATASET\_REGISTRY：以 \_dataset.py 结尾

2、ARCH\_REGISTRY：以 \_arch.py 结尾

3、MODEL\_REGISTRY：以 \_model.py 结尾

4、LOSS\_REGISTRY：以 \_loss.py 结尾 (目前只有一个文件，所以还没有添加相应的扫描的代码，后面会添加上)

5、METRIC\_REGISTRY: 这个因为改动很少，我们就保留了 在\_\_init\_\_.py 文件中 import 的方式:
\begin{minted}[xleftmargin=20pt,linenos,bgcolor=bg,breaklines]{python}
from copy import deepcopy

from basicsr.utils.registry import METRIC_REGISTRY
from .niqe import calculate_niqe
from .psnr_ssim import calculate_psnr, calculate_ssim

__all__ = ['calculate_psnr', 'calculate_ssim', 'calculate_niqe']


def calculate_metric(data, opt):
    """Calculate metric from data and options.
    Args:
        opt (dict): Configuration. It must contain:
            type (str): Model type.
    """
    opt = deepcopy(opt)
    metric_type = opt.pop('type')
    metric = METRIC_REGISTRY.get(metric_type)(**data, **opt)
    return metric
\end{minted}

总结一下，当我们在新开发网络结构的时候，只要做两件事，修改两个文件就好了。BasicSR 背后的动态实例化和 REGISTRY 机制会帮你完成剩下的事。

1、写一个单独的网络结构文件 (以 \_arch.py 结尾)。在写好的 Class 前加上 @ARCH\_REGISTRY.register() 装饰器。

2、在配置文件中指定使用哪一个网络结构，即上面的 Class name。



1、无法避免重名的问题。一旦出现了类名重名，我们可能无法知道是否实例化了自己想要的类。2、该机制会把文件中所有的类、函数都 import 进来，十分冗余，因为很多类和函数都是中间的量。

具体而言，我们是通过importlib和getattr来实现的。以 data 为例，我们在 \href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/data/__init__.py}{data/\_\_init\_\_.py} 中是如下做的:

1、扫描所有以 \_dataset.py 为结尾的文件 (这是约定)；

2、把这些文件中的类或函数通过 importlib 都 import 进来；

3、根据配置文件中的名称，通过 getattr 实例化。

具体操作的代码如下：

(读者只需知道这个机制即可，以下代码不影响 BasicSR 的直接使用)
\begin{minted}[xleftmargin=20pt,linenos,bgcolor=bg,breaklines]{python}
# automatically scan and import dataset modules
# scan all the files under the data folder with '_dataset' in file names
data_folder = osp.dirname(osp.abspath(__file__))
dataset_filenames = [
    osp.splitext(osp.basename(v))[0] for v in scandir(data_folder)
    if v.endswith('_dataset.py')
]
# import all the dataset modules
_dataset_modules = [
    importlib.import_module(f'basicsr.data.{file_name}')
    for file_name in dataset_filenames
]

...

# dynamic instantiation
for module in _dataset_modules:
    dataset_cls = getattr(module, dataset_type, None)
    if dataset_cls is not None:
        break
\end{minted}

我们对以下模块使用了类似的技巧，在使用的时候需要注意文件后缀名称的约定：
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Module} & \textbf{File Suffix} & \textbf{Example} \\ \hline
Data & \_dataset.py & data/paired\_image\_dataset.py \\ \hline
Model & \_model.py & basicsr/models/sr\_model.py \\ \hline
Archs & \_arch.py & basicsr/archs/srresnet\_arch.py \\ \hline
\end{tabular}
\caption{动态实例化文件命名约定}
\end{table}

\begin{hl} % ---------------- Highlight block ---------------- %
\textbf{注意}

1、上面的文件后缀只用在需要的文件中，其他文件命名尽量避免使用以上的后缀。

2、类名或函数名不能重复。
\end{hl}

另外，对 losses 和 metrics，我们也使用了 importlib 和 getattr，但是和上面不一样的是，对于 losses 和 metrics，由于文件数量比较少，改动也少，因此我们不采用扫描文件的方式，而是在新增加类/函数后，需要在相应的 \_\_init\_\_.py 中增加类/函数名称。
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Module} & \textbf{Path} & \textbf{Modify} \\ \hline
Losses & basicsr/models/losses & basicsr/models/losses/\_\_init\_\_.py \\ \hline
Metrics & basicsr/metrics & basicsr/metrics/\_\_init\_\_.py \\ \hline
Archs & \_arch.py & basicsr/archs/srresnet\_arch.py \\ \hline
\end{tabular}
\caption{部分类的实例化需要修改对应的 \_\_init\_\_.py 文件}
\end{table}

在 log 的时候, loss 项使用 l\_ 开头，这样在 Tensorboard 显示的时候，所有 loss 会被组织到一起。比如在 \href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/models/srgan_model.py}{basicsr/models/srgan\_model.py} 中，使用了 l\_g\_pix，l\_g\_percep，l\_g\_gan 等。在 \href{https://github.com/XPixelGroup/BasicSR/blob/master/basicsr/utils/logger.py}{basicsr/utils/logger.py} 中，他们会被组织到一起：
\begin{minted}[xleftmargin=20pt,bgcolor=bg,breaklines]{python}
if k.startswith('l_'):
    self.tb_logger.add_scalar(f'losses/{k}', v, current_iter)
else:
    self.tb_logger.add_scalar(k, v, current_iter)
\end{minted}





% ------------------------------------------------------------------------------

\section{数据 (Data Loader)}\label{code_structure:data}
    数据是机器学习的动力来源。这一章节我们介绍 BasicSR 的数据读取和处理机制。在 \href{https://github.com/XPixelGroup/BasicSR/tree/master/basicsr/data}{basicsr/data/} 目录下，我们提供了常用的 Dataset 文件。

    \dirtree{%
        .1 ROOT\_DIR.
        .2 BasicSR.
        .3 basicsr.
        .4 data.
        .5 \_\_init\_\_.py.
        .5 paired\_image\_dataset.py.
        .5 single\_image\_dataset.py.
        .5 realesrgan\_paired\_dataset.py.
        .5 realesrgan\_dataset.py.
        .5 reds\_dataset.py.
        .5 vimeo90k\_dataset.py.
        .5 video\_test\_dataset.py.
        .5 ffhq\_dataset.py.
        .5 data\_util.py.\DTcomment{提供了数据读取相关的函数}.
        .5 data\_sampler.py.
        .5 degradations.py.\DTcomment{提供若干图像退化的合成函数}.
        .5 prefetch\_dataloader.py.
        .5 transforms.py.\DTcomment{提供了常用的数据增强函数}.
    }

    BasicSR 提供的常用数据集的 Dataset 文件如下：

    \begin{table}[h]
    \centering
    \resizebox{\textwidth}{26mm}{
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{类} & \textbf{任务} & \textbf{训练/测试} & \textbf{描述} \\ \hline
    PairedImageDataset & 图像超分 & 训练 & 读取成对的训练数据 \\ \hline
    SingleImageDataset & 图像超分 & 测试 & 只读取 low quality 的图像, 用于没有 GT 的测试中 \\ \hline
    REDSDataset & 视频超分 & 训练 & 读取 REDS 的训练数据集 \\ \hline
    Vimeo90KDataset & 视频超分 & 训练 & 读取 Vimeo90K 的训练数据集 \\ \hline
    VideoTestDataset & 视频超分 & 测试 & 基础的视频超分测试集, 支持 Vid4,  REDS 测试集 \\ \hline
    VideoTestVimeo90KDataset & 视频超分 & 测试 & 继承 VideoTestDataset；Vimeo90K 的测试数据集 \\ \hline
    VideoTestDUFDataset & 视频超分 & 测试 & 继承 VideoTestDataset； DUF算法的测试数据集, 支持 Vid4 \\ \hline
    FFHQDataset & 人脸生成 & 训练 & 读取 FFHQ 的训练数据集 \\ \hline
    \end{tabular}
    }
    \caption{BasicSR 提供的数据处理类}
    \end{table}

    下面，我们以 PairedImageDataset 为例，大致讲解 Dataset 文件的内容。
    \begin{minted}[xleftmargin=20pt,linenos,breaklines,bgcolor=bg,breaklines]{python}
    @DATASET_REGISTRY.register()
    class PairedImageDataset(data.Dataset):
        def __init__(self, opt):
            super(PairedImageDataset, self).__init__()
            self.opt = opt
            # file client (io backend)
            self.file_client = None
            self.io_backend_opt = opt['io_backend']
            self.mean = opt['mean'] if 'mean' in opt else None
            self.std = opt['std'] if 'std' in opt else None
    \end{minted}

    在 option 文件中可以指定数据读取方式 (opt['io\_backend'])。

    我们支持三种读取数据的模式：

    1、直接从 lmdb 格式的文件中读取；

    2、若提供了 meta\_info 文件，则直接从该文件中列出的文件路径读取数据；

    3、输入文件目录，代码会自动扫描该目录中的文件，然后读取。


    \begin{minted}[xleftmargin=20pt,linenos,breaklines,bgcolor=bg]{python}
            self.gt_folder, self.lq_folder = opt['dataroot_gt'], opt['dataroot_lq']
            if 'filename_tmpl' in opt:
                self.filename_tmpl = opt['filename_tmpl']
            else:
                self.filename_tmpl = '{}'

            if self.io_backend_opt['type'] == 'lmdb':
                self.io_backend_opt['db_paths'] = [self.lq_folder, self.gt_folder]
                self.io_backend_opt['client_keys'] = ['lq', 'gt']
                self.paths = paired_paths_from_lmdb([self.lq_folder, self.gt_folder], ['lq', 'gt'])
            elif 'meta_info_file' in self.opt and self.opt['meta_info_file'] is not None:
                self.paths = paired_paths_from_meta_info_file([self.lq_folder, self.gt_folder], ['lq', 'gt'], self.opt['meta_info_file'], self.filename_tmpl)
            else:
                self.paths = paired_paths_from_folder([self.lq_folder, self.gt_folder], ['lq', 'gt'], self.filename_tmpl)
    \end{minted}

    从 option 文件中读取 GT 图像目录 (opt['dataroot\_gt']) 和输入图像目录 (opt['dataroot\_lq'])。

    根据指定的文件读取方式，选择相应的读取函数。

    1、lmdb 方式， 选择 paired\_paths\_from\_lmdb 函数;

    2、meta\_info\_file 方式，选择 paired\_paths\_from\_meta\_info\_file 函数；

    3、一般的文件目录方式，选择 paired\_paths\_from\_folder 函数。

    \begin{minted}[xleftmargin=20pt,linenos,breaklines,bgcolor=bg]{python}
        def __getitem__(self, index):
            if self.file_client is None:
                self.file_client = FileClient(self.io_backend_opt.pop('type'), **self.io_backend_opt)

            scale = self.opt['scale']

            # Load gt and lq images. Dimension order: HWC; channel order: BGR;
            # image range: [0, 1], float32.
            gt_path = self.paths[index]['gt_path']
            img_bytes = self.file_client.get(gt_path, 'gt')
            img_gt = imfrombytes(img_bytes, float32=True)
            lq_path = self.paths[index]['lq_path']
            img_bytes = self.file_client.get(lq_path, 'lq')
            img_lq = imfrombytes(img_bytes, float32=True)
    \end{minted}

    \_\_getitem\_\_() 函数是 Dataset 中最关键的函数，定义了每一次迭代时数据是如何读取并处理的。 图像通过 FileClient 读取后，shape 为 HWC， 其中颜色通道的排列顺序为 BGR。图像此时为 [0, 1] 的 float32 格式。

    \begin{minted}[xleftmargin=20pt,linenos,breaklines,bgcolor=bg]{python}
            # augmentation for training
            if self.opt['phase'] == 'train':
                gt_size = self.opt['gt_size']
                # random crop
                img_gt, img_lq = paired_random_crop(img_gt, img_lq, gt_size, scale, gt_path)
                # flip, rotation
                img_gt, img_lq = augment([img_gt, img_lq], self.opt['use_hflip'], self.opt['use_rot'])

            # color space transform
            if 'color' in self.opt and self.opt['color'] == 'y':
                img_gt = rgb2ycbcr(img_gt, y_only=True)[..., None]
                img_lq = rgb2ycbcr(img_lq, y_only=True)[..., None]
    \end{minted}

    这一部分首先对读取的图像进行 crop 处理 (从原图中随机 crop 出一个 patch)，然后进行随机的水平翻转和旋转，进行数据增强。如果要使用单通道训练 (YCbCR 颜色空间中的 Y 通道)，代码会对 GT 和 input 进行颜色转换。

    \begin{minted}[xleftmargin=20pt,linenos,breaklines,bgcolor=bg]{python}
            # crop the unmatched GT images during validation or testing, especially for SR benchmark datasets
            if self.opt['phase'] != 'train':
                img_gt = img_gt[0:img_lq.shape[0] * scale, 0:img_lq.shape[1] * scale, :]

            # BGR to RGB, HWC to CHW, numpy to tensor
            img_gt, img_lq = img2tensor([img_gt, img_lq], bgr2rgb=True, float32=True)
            # normalize
            if self.mean is not None or self.std is not None:
                normalize(img_lq, self.mean, self.std, inplace=True)
                normalize(img_gt, self.mean, self.std, inplace=True)

            return {'lq': img_lq, 'gt': img_gt, 'lq_path': lq_path, 'gt_path': gt_path}
    \end{minted}

    在测试时，由于上下采样的原因，有时候模型的输出和原始的图像大小会出现不匹配的情况。例如，原始图像大小为 $530 \times 530$，在 $\times 4$  超分中，原图会下采样变成 $132 \times 132$ 的输入，模型会超分后会得到 $528 \times 528$ 的输出，与原图大小不匹配，进而无法直接计算 PSNR 等指标。于是，为了避免这个问题，我们在测试时，会 crop 原始图像多余的像素，使其分辨率和模型输出相同。

    另外，我们对图像格式进行一些转换：把 BGR 格式转换为 RGB；把 HWC 的排列，转换为 Pytorch所需的 CHW。如果指定了数据集的均值 (mean) 和标准差 (std)，我们将会进一步对数据进行Z-score标准化操作。

    最后，我们返回一个字典，包括输入的 LQ 图像，作为标签的 GT 图像，以及他们的路径。

% ------------------------------------------------------------------------------

\section{网络结构 (Architecture)} \label{code_structure:arch}
    在 \href{https://github.com/XPixelGroup/BasicSR/tree/master/basicsr/archs}{basicsr/archs/} 目录下，我们提供了若干经典的网络结构。

    \begin{table}[h]
    \centering
    {
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{网络结构} & \textbf{任务} & \textbf{文件} & \textbf{描述} \\ \hline
    EDSR & 图像超分 & edsr\_arch.py & 论文EDSR结构 \\ \hline
    SRResNet & 图像超分 & srresnet\_arch.py & 论文SRGAN的Generator \\ \hline
    RRDB & 图像超分 & rrdbnet\_arch.py & 论文ESRGAN的Generator \\ \hline
    RCAN & 图像超分 & rcan\_arch.py & 论文RCAN的结构 \\ \hline
    SwinIR & 图像超分 & swinir\_arch.py & 论文SwinIR的结构 \\ \hline
    ECBSR & 图像超分 & ecbsr\_arch.py & 论文ECBSR的结构 \\ \hline
    SRVGG & 图像超分 & srvgg\_arch.py & VGG结构改造的SR网络 \\ \hline
    EDVR & 视频超分 & edvr\_arch.py & 论文EDVR的结构 \\ \hline
    BasicVSR & 视频超分 & basicvsr\_arch.py &  论文BasicVSR的结构\\ \hline
    BasicVSR++ & 视频超分 & basicvsrpp\_arch.py &  论文BasicVSR++的结构\\ \hline
    DUF & 视频超分 & duf\_arch.py &  论文DUF的结构\\ \hline
    TOF & 视频超分 & tof\_arch.py &  论文TOF的结构\\ \hline
    HiFaceGAN & 人脸生成 & hifacegan\_arch.py &  论文HiFaceGAN的结构\\ \hline
    DFDNet & 人脸生成 & dfdnet\_arch.py &  论文Deep Face Dictionary Network的结构\\ \hline
    StyleGAN2  & 人脸生成 & stylegan2\_arch.py &  论文StyleGAN2的结构 \\ \hline
    RIDNet  & 图像去噪 & ridnet\_arch\_arch.py &  论文RIDNet的结构 \\ \hline
    VGG  & 工具结构 & vgg\_arch.py &  经典VGG结构 \\ \hline
    InceptionV3  & 工具结构 & inception.py &  经典InceptionV3的结构，用于计算FID指标 \\ \hline
    VGG \& UNet  & 工具结构 & discriminator\_arch.py &  在GAN训练中，常用的Discriminator结构 \\ \hline
    SpyNet  & 工具结构 & spynet\_arch.py &  论文SpyNet的结构 \\ \hline
    \end{tabular}
    }
    \caption{BasicSR 提供的经典网络结构}
    \end{table}



\section{模型 (Model)}\label{code_structure:model}

- Base模型

- 其他模型

- 继承关系

\section{损失函数 (Loss)}\label{code_structure:loss}

\section{训练(优化器与学习率调度器)}\label{code_structure:training}

\section{算子}\label{code_structure:ops}

\end{document}